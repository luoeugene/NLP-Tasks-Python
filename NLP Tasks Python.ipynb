{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# This notebook will use NLTK library for natural language processing tasks.\n",
    "Created by Eugene Luo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Q1)\n",
    "# A) First install the NLTK package using the computer terminal:\n",
    "pip install nltk"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# B) Download the Gutenberg corpus tool in NLTK package"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package gutenberg to\n",
      "[nltk_data]     C:\\Users\\luoeu\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package gutenberg is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('gutenberg')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# C) Use text in the corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import gutenberg #import corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['austen-emma.txt',\n",
       " 'austen-persuasion.txt',\n",
       " 'austen-sense.txt',\n",
       " 'bible-kjv.txt',\n",
       " 'blake-poems.txt',\n",
       " 'bryant-stories.txt',\n",
       " 'burgess-busterbrown.txt',\n",
       " 'carroll-alice.txt',\n",
       " 'chesterton-ball.txt',\n",
       " 'chesterton-brown.txt',\n",
       " 'chesterton-thursday.txt',\n",
       " 'edgeworth-parents.txt',\n",
       " 'melville-moby_dick.txt',\n",
       " 'milton-paradise.txt',\n",
       " 'shakespeare-caesar.txt',\n",
       " 'shakespeare-hamlet.txt',\n",
       " 'shakespeare-macbeth.txt',\n",
       " 'whitman-leaves.txt']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.corpus.gutenberg.fileids() #see the file ids in gutenberg corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "154883"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "whitman = nltk.corpus.gutenberg.words('whitman-leaves.txt') # Assign words in the text to variable\n",
    "len(whitman) #number of words in the text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['[', 'Leaves', 'of', 'Grass', 'by', 'Walt', 'Whitman', ...]\n"
     ]
    }
   ],
   "source": [
    "print(whitman) # see the text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "18"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(gutenberg.fileids()) # Number of sources in Gutenberg"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# D) Create a table displaying relative frequencies for (can, could, may, might, will, would, and should) in the corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                           can  could    may  might   will  would should \n",
      "        austen-emma.txt    270    825    213    322    559    815    366 \n",
      "  austen-persuasion.txt    100    444     87    166    162    351    185 \n",
      "       austen-sense.txt    206    568    169    215    354    507    228 \n",
      "          bible-kjv.txt    213    165   1024    475   3807    443    768 \n",
      "        blake-poems.txt     20      3      5      2      3      3      6 \n",
      "     bryant-stories.txt     75    154     18     23    144    110     38 \n",
      "burgess-busterbrown.txt     23     56      3     17     19     46     13 \n",
      "      carroll-alice.txt     57     73     11     28     24     70     27 \n",
      "    chesterton-ball.txt    131    117     90     69    198    139     75 \n",
      "   chesterton-brown.txt    126    170     47     71    111    132     56 \n",
      "chesterton-thursday.txt    117    148     56     71    109    116     54 \n",
      "  edgeworth-parents.txt    340    420    160    127    517    503    271 \n",
      " melville-moby_dick.txt    220    215    230    183    379    421    181 \n",
      "    milton-paradise.txt    107     62    116     98    161     49     55 \n",
      " shakespeare-caesar.txt     16     18     35     12    129     40     38 \n",
      " shakespeare-hamlet.txt     33     26     56     28    131     60     52 \n",
      "shakespeare-macbeth.txt     21     15     30      5     62     42     41 \n",
      "     whitman-leaves.txt     88     49     85     26    261     85     42 \n"
     ]
    }
   ],
   "source": [
    "cfd = nltk.ConditionalFreqDist((file, word)\n",
    "                              for file in gutenberg.fileids()\n",
    "                              for word in gutenberg.words(fileids=file))\n",
    "files = ['austen-emma.txt',\n",
    " 'austen-persuasion.txt',\n",
    " 'austen-sense.txt',\n",
    " 'bible-kjv.txt',\n",
    " 'blake-poems.txt',\n",
    " 'bryant-stories.txt',\n",
    " 'burgess-busterbrown.txt',\n",
    " 'carroll-alice.txt',\n",
    " 'chesterton-ball.txt',\n",
    " 'chesterton-brown.txt',\n",
    " 'chesterton-thursday.txt',\n",
    " 'edgeworth-parents.txt',\n",
    " 'melville-moby_dick.txt',\n",
    " 'milton-paradise.txt',\n",
    " 'shakespeare-caesar.txt',\n",
    " 'shakespeare-hamlet.txt',\n",
    " 'shakespeare-macbeth.txt',\n",
    " 'whitman-leaves.txt']\n",
    "modals = ['can', 'could', 'may', 'might', 'will', 'would', 'should']\n",
    "cfd.tabulate(conditions=files, samples=modals)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# E) Select the largest span of relative freq of most used modals minus least used modals. Compare usage in the texts. Try to suggest an explanation for why those words are used differently in the two texts."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Most used modal in a text: 'will' : 3,807    (bible-kjv.txt)\n",
    "\n",
    "Least used modal in a text: 'might' : 2      (blake-poems.txt)\n",
    "\n",
    "The most used modal for 'will' is found in the Bible. I think the explanation for this is because the Bible often tells us what God will do in certain events. Additionally, the word, 'will', can be used to mean 'God's will' as well. Therefore, it makes sense that this modal is used more often in the Bible.\n",
    "\n",
    "The least used modal, 'might', is found in blake poems. After inspecting the text, it seems to be a short poem describing what what two people are doing. In this case, since the actions described are declarative, you won't often find the word, 'might', in these sentences since these actions have been done already. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[The King James Bible]\n",
      "\n",
      "The Old Testament of the King James Bible\n",
      "\n",
      "The First Book of Moses:  Called Genesis\n",
      "\n",
      "\n",
      "1:1 In the beginning God created the heaven and the earth.\n",
      "\n",
      "1:2 And the earth was without form, and void; and darkness was upon\n",
      "the face of the deep. And the Spirit of God moved upon the face of the\n",
      "waters.\n",
      "\n",
      "1:3 And God said, Let there be light: and there was light.\n",
      "\n",
      "1:4 And God saw the light, that it was good: and God divided the light\n",
      "from the darkness.\n",
      "\n",
      "1:5 And God called the light Day\n"
     ]
    }
   ],
   "source": [
    "print(gutenberg.raw('bible-kjv.txt').strip()[:501]) # First 501 characters. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Poems by William Blake 1789]\n",
      "\n",
      " \n",
      "SONGS OF INNOCENCE AND OF EXPERIENCE\n",
      "and THE BOOK of THEL\n",
      "\n",
      "\n",
      " SONGS OF INNOCENCE\n",
      " \n",
      " \n",
      " INTRODUCTION\n",
      " \n",
      " Piping down the valleys wild,\n",
      "   Piping songs of pleasant glee,\n",
      " On a cloud I saw a child,\n",
      "   And he laughing said to me:\n",
      " \n",
      " \"Pipe a song about a Lamb!\"\n",
      "   So I piped with merry cheer.\n",
      " \"Piper, pipe that song again;\"\n",
      "   So I piped: he wept to hear.\n",
      " \n",
      " \"Drop thy pipe, thy happy pipe;\n",
      "   Sing thy songs of happy cheer:!\"\n",
      " So I sang the same again,\n",
      "   While he wept with\n"
     ]
    }
   ],
   "source": [
    "print(gutenberg.raw('blake-poems.txt').strip()[:502]) # First 502 characters."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# # Q2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# A) Import inaugural corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package inaugural to\n",
      "[nltk_data]     C:\\Users\\luoeu\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package inaugural is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "nltk.download('inaugural')\n",
    "from nltk.corpus import inaugural"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# B) Choose Kennedy's speech"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Vice', 'President', 'Johnson', ',', 'Mr', '.', ...]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Kennedy = nltk.corpus.inaugural.words('1961-Kennedy.txt')\n",
    "Kennedy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# C) Find 10 most frequently used long words (> 7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Words</th>\n",
       "      <th>Count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>citizens</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>president</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>americans</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>generation</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>forebears</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>revolution</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>committed</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>powerful</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>supporting</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>themselves</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        Words  Count\n",
       "0    citizens      5\n",
       "1   president      4\n",
       "2   americans      4\n",
       "3  generation      3\n",
       "4   forebears      2\n",
       "5  revolution      2\n",
       "6   committed      2\n",
       "7    powerful      2\n",
       "8  supporting      2\n",
       "9  themselves      2"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk import FreqDist\n",
    "import pandas as pd\n",
    "\n",
    "freqchar = ([w.lower() for w in Kennedy if len(w) > 7]) #Filter only letters over 7 characters\n",
    "\n",
    "# Make a frequency distribution\n",
    "fdist = FreqDist(freqchar)\n",
    "freq = fdist.most_common(10)\n",
    "\n",
    "#create arrays\n",
    "np1 = []\n",
    "np2 = []\n",
    "\n",
    "# Seperate words\n",
    "for i in range(10):\n",
    "    np1.append(freq[i][0])\n",
    "    \n",
    "for i in range(10):\n",
    "    np2.append(freq[i][1])\n",
    "\n",
    "# Create data frame\n",
    "df_freq = pd.DataFrame({'Words':np1, 'Count':np2})\n",
    "df_freq #print data frame"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# D) Which word has the largest number of synonyms using WordNet?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\luoeu\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('wordnet')\n",
    "\n",
    "from nltk.corpus import wordnet as wn\n",
    "import numpy as np\n",
    "\n",
    "np_syn = [] #create array\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word:  citizens\n",
      "Synonyms: \n",
      "  ['citizen']\n",
      "Word:  president\n",
      "Synonyms: \n",
      "  ['president']\n",
      "  ['President_of_the_United_States', 'United_States_President', 'President', 'Chief_Executive']\n",
      "  ['president']\n",
      "  ['president', 'chairman', 'chairwoman', 'chair', 'chairperson']\n",
      "  ['president', 'prexy']\n",
      "  ['President_of_the_United_States', 'President', 'Chief_Executive']\n",
      "Word:  americans\n",
      "Synonyms: \n",
      "  ['American']\n",
      "  ['American_English', 'American_language', 'American']\n",
      "  ['American']\n",
      "Word:  generation\n",
      "Synonyms: \n",
      "  ['coevals', 'contemporaries', 'generation']\n",
      "  ['generation']\n",
      "  ['generation']\n",
      "  ['generation']\n",
      "  ['genesis', 'generation']\n",
      "  ['generation']\n",
      "  ['generation', 'multiplication', 'propagation']\n",
      "Word:  forebears\n",
      "Synonyms: \n",
      "  ['forebear', 'forbear']\n",
      "Word:  revolution\n",
      "Synonyms: \n",
      "  ['revolution']\n",
      "  ['revolution']\n",
      "  ['rotation', 'revolution', 'gyration']\n",
      "Word:  committed\n",
      "Synonyms: \n",
      "  ['perpetrate', 'commit', 'pull']\n",
      "  ['give', 'dedicate', 'consecrate', 'commit', 'devote']\n",
      "  ['commit', 'institutionalize', 'institutionalise', 'send', 'charge']\n",
      "  ['entrust', 'intrust', 'trust', 'confide', 'commit']\n",
      "  ['invest', 'put', 'commit', 'place']\n",
      "  ['commit', 'practice']\n",
      "  ['committed']\n",
      "  ['attached', 'committed']\n",
      "Word:  powerful\n",
      "Synonyms: \n",
      "  ['powerful']\n",
      "  ['knock-down', 'powerful']\n",
      "  ['potent', 'powerful']\n",
      "  ['brawny', 'hefty', 'muscular', 'powerful', 'sinewy']\n",
      "  ['herculean', 'powerful']\n",
      "  ['mighty', 'mightily', 'powerful', 'right']\n",
      "Word:  supporting\n",
      "Synonyms: \n",
      "  ['support', 'supporting']\n",
      "  ['support', 'back_up']\n",
      "  ['support']\n",
      "  ['back', 'endorse', 'indorse', 'plump_for', 'plunk_for', 'support']\n",
      "  ['hold', 'support', 'sustain', 'hold_up']\n",
      "  ['confirm', 'corroborate', 'sustain', 'substantiate', 'support', 'affirm']\n",
      "  ['subscribe', 'support']\n",
      "  ['corroborate', 'underpin', 'bear_out', 'support']\n",
      "  ['defend', 'support', 'fend_for']\n",
      "  ['support']\n",
      "  ['patronize', 'patronise', 'patronage', 'support', 'keep_going']\n",
      "  ['digest', 'endure', 'stick_out', 'stomach', 'bear', 'stand', 'tolerate', 'support', 'brook', 'abide', 'suffer', 'put_up']\n",
      "  ['encouraging', 'supporting']\n",
      "  ['load-bearing', 'supporting']\n",
      "Word:  themselves\n",
      "Synonyms: \n"
     ]
    }
   ],
   "source": [
    "# loop through each word\n",
    "\n",
    "for i in fdist.most_common(10):\n",
    "    print('Word: ', i[0])\n",
    "    count = 0\n",
    "    \n",
    "    #Print the synonym\n",
    "    print('Synonyms: ')\n",
    "    for j in wn.synsets(i[0]):\n",
    "        print(' ', j.lemma_names())\n",
    "        count+=len(j.lemma_names())\n",
    "    np_syn.append([i[0], count])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create empty arrays\n",
    "np_1 = []\n",
    "np_2 = []\n",
    "\n",
    "# Get the synonyms\n",
    "for i in range(10):\n",
    "    np_1.append(np_syn[i][0])\n",
    "    \n",
    "for j in range(10):\n",
    "    np_2.append(np_syn[j][1])\n",
    "\n",
    "#Create data frame\n",
    "df_syn = pd.DataFrame({'Words':np_1, 'Count':np_2})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# E) List all synonyms for the 10 most frequently used words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Words</th>\n",
       "      <th>Count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>supporting</td>\n",
       "      <td>52</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>committed</td>\n",
       "      <td>27</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>president</td>\n",
       "      <td>16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>powerful</td>\n",
       "      <td>16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>generation</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>americans</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>revolution</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>forebears</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>citizens</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>themselves</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        Words  Count\n",
       "8  supporting     52\n",
       "6   committed     27\n",
       "1   president     16\n",
       "7    powerful     16\n",
       "3  generation     12\n",
       "2   americans      5\n",
       "5  revolution      5\n",
       "4   forebears      2\n",
       "0    citizens      1\n",
       "9  themselves      0"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#sort from highest to lowest\n",
    "df_syn.sort_values(by=\"Count\", ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "'supporting' has the largest number of synonyms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# F) Which one of the 10 words has the largest number of hyponyms?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word:  citizens\n",
      "Synset('citizen.n.01')\n",
      "[Synset('active_citizen.n.01'), Synset('civilian.n.01'), Synset('freeman.n.01'), Synset('private_citizen.n.01'), Synset('repatriate.n.01'), Synset('thane.n.02'), Synset('voter.n.01')]\n",
      "Word:  president\n",
      "Synset('president.n.01')\n",
      "[]\n",
      "Synset('president_of_the_united_states.n.01')\n",
      "[]\n",
      "Synset('president.n.03')\n",
      "[Synset('ex-president.n.01')]\n",
      "Synset('president.n.04')\n",
      "[Synset('kalon_tripa.n.01'), Synset('vice_chairman.n.01')]\n",
      "Synset('president.n.05')\n",
      "[]\n",
      "Synset('president_of_the_united_states.n.02')\n",
      "[]\n",
      "Word:  americans\n",
      "Synset('american.n.01')\n",
      "[Synset('african-american.n.01'), Synset('alabaman.n.01'), Synset('alaskan.n.01'), Synset('anglo-american.n.01'), Synset('appalachian.n.01'), Synset('arizonan.n.01'), Synset('arkansan.n.01'), Synset('asian_american.n.01'), Synset('bay_stater.n.01'), Synset('bostonian.n.01'), Synset('californian.n.01'), Synset('carolinian.n.01'), Synset('coloradan.n.01'), Synset('connecticuter.n.01'), Synset('creole.n.02'), Synset('delawarean.n.01'), Synset('floridian.n.01'), Synset('franco-american.n.01'), Synset('georgian.n.01'), Synset('german_american.n.01'), Synset('hawaiian.n.02'), Synset('idahoan.n.01'), Synset('illinoisan.n.01'), Synset('indianan.n.01'), Synset('iowan.n.01'), Synset('kansan.n.01'), Synset('kentuckian.n.01'), Synset('louisianan.n.01'), Synset('mainer.n.01'), Synset('marylander.n.01'), Synset('michigander.n.01'), Synset('minnesotan.n.01'), Synset('mississippian.n.02'), Synset('missourian.n.01'), Synset('montanan.n.01'), Synset('nebraskan.n.01'), Synset('nevadan.n.01'), Synset('new_englander.n.01'), Synset('new_hampshirite.n.01'), Synset('new_jerseyan.n.01'), Synset('new_mexican.n.01'), Synset('new_yorker.n.01'), Synset('nisei.n.01'), Synset('north_carolinian.n.01'), Synset('north_dakotan.n.01'), Synset('ohioan.n.01'), Synset('oklahoman.n.01'), Synset('oregonian.n.01'), Synset('pennsylvanian.n.02'), Synset('puerto_rican.n.01'), Synset('rhode_islander.n.01'), Synset('south_carolinian.n.01'), Synset('south_dakotan.n.01'), Synset('southerner.n.01'), Synset('spanish_american.n.01'), Synset('tennessean.n.01'), Synset('texan.n.01'), Synset('tory.n.01'), Synset('utahan.n.01'), Synset('vermonter.n.01'), Synset('virginian.n.01'), Synset('washingtonian.n.01'), Synset('washingtonian.n.02'), Synset('west_virginian.n.01'), Synset('wisconsinite.n.01'), Synset('wyomingite.n.01'), Synset('yankee.n.01'), Synset('yankee.n.03')]\n",
      "Synset('american_english.n.01')\n",
      "[Synset('african_american_vernacular_english.n.01')]\n",
      "Synset('american.n.03')\n",
      "[Synset('creole.n.01'), Synset('latin_american.n.01'), Synset('mesoamerican.n.01'), Synset('north_american.n.01'), Synset('south_american.n.01'), Synset('west_indian.n.01')]\n",
      "Word:  generation\n",
      "Synset('coevals.n.01')\n",
      "[Synset('peer_group.n.01'), Synset('youth_culture.n.01')]\n",
      "Synset('generation.n.02')\n",
      "[Synset('baby_boom.n.01'), Synset('generation_x.n.01'), Synset('posterity.n.02')]\n",
      "Synset('generation.n.03')\n",
      "[]\n",
      "Synset('generation.n.04')\n",
      "[]\n",
      "Synset('genesis.n.01')\n",
      "[]\n",
      "Synset('generation.n.06')\n",
      "[]\n",
      "Synset('generation.n.07')\n",
      "[Synset('biogenesis.n.02')]\n",
      "Word:  forebears\n",
      "Synset('forebear.n.01')\n",
      "[Synset('grandparent.n.01'), Synset('great_grandparent.n.01')]\n",
      "Word:  revolution\n",
      "Synset('revolution.n.01')\n",
      "[Synset('cultural_revolution.n.01'), Synset('green_revolution.n.01')]\n",
      "Synset('revolution.n.02')\n",
      "[Synset('counterrevolution.n.01')]\n",
      "Synset('rotation.n.03')\n",
      "[Synset('axial_rotation.n.01'), Synset('dextrorotation.n.01'), Synset('levorotation.n.01'), Synset('orbital_rotation.n.01'), Synset('spin.n.01')]\n",
      "Word:  committed\n",
      "Synset('perpetrate.v.01')\n",
      "[Synset('make.v.24'), Synset('recommit.v.01')]\n",
      "Synset('give.v.18')\n",
      "[Synset('apply.v.10'), Synset('rededicate.v.01'), Synset('vow.v.02')]\n",
      "Synset('commit.v.03')\n",
      "[Synset('hospitalize.v.01')]\n",
      "Synset('entrust.v.01')\n",
      "[Synset('commend.v.03'), Synset('consign.v.02'), Synset('obligate.v.02'), Synset('recommit.v.02')]\n",
      "Synset('invest.v.01')\n",
      "[Synset('buy_into.v.01'), Synset('fund.v.04'), Synset('roll_over.v.03'), Synset('shelter.v.02'), Synset('speculate.v.04'), Synset('tie_up.v.02')]\n",
      "Synset('commit.v.06')\n",
      "[]\n",
      "Synset('committed.a.01')\n",
      "[]\n",
      "Synset('attached.a.03')\n",
      "[]\n",
      "Word:  powerful\n",
      "Synset('powerful.a.01')\n",
      "[]\n",
      "Synset('knock-down.s.01')\n",
      "[]\n",
      "Synset('potent.s.01')\n",
      "[]\n",
      "Synset('brawny.s.01')\n",
      "[]\n",
      "Synset('herculean.s.01')\n",
      "[]\n",
      "Synset('mighty.r.01')\n",
      "[]\n",
      "Word:  supporting\n",
      "Synset('support.n.08')\n",
      "[Synset('shoring.n.02'), Synset('suspension.n.06')]\n",
      "Synset('support.v.01')\n",
      "[Synset('help.v.01'), Synset('patronize.v.02'), Synset('promote.v.01'), Synset('second.v.01'), Synset('sponsor.v.02'), Synset('undergird.v.01')]\n",
      "Synset('support.v.02')\n",
      "[Synset('fund.v.06'), Synset('provide.v.06'), Synset('see_through.v.01'), Synset('sponsor.v.01'), Synset('subsidize.v.01')]\n",
      "Synset('back.v.01')\n",
      "[Synset('champion.v.01'), Synset('guarantee.v.04')]\n",
      "Synset('hold.v.10')\n",
      "[Synset('block.v.11'), Synset('brace.v.03'), Synset('bracket.v.01'), Synset('buoy.v.02'), Synset('carry.v.05'), Synset('chock.v.02'), Synset('pole.v.02'), Synset('prop_up.v.01'), Synset('scaffold.v.01'), Synset('truss.v.03'), Synset('underpin.v.01')]\n",
      "Synset('confirm.v.01')\n",
      "[Synset('back.v.09'), Synset('document.v.02'), Synset('prove.v.02'), Synset('validate.v.02'), Synset('verify.v.01'), Synset('vouch.v.04')]\n",
      "Synset('subscribe.v.03')\n",
      "[]\n",
      "Synset('corroborate.v.03')\n",
      "[]\n",
      "Synset('defend.v.01')\n",
      "[Synset('apologize.v.02'), Synset('stand_up.v.05'), Synset('uphold.v.02')]\n",
      "Synset('support.v.09')\n",
      "[]\n",
      "Synset('patronize.v.04')\n",
      "[]\n",
      "Synset('digest.v.03')\n",
      "[Synset('accept.v.07'), Synset('bear_up.v.01'), Synset('pay.v.09'), Synset('sit_out.v.02'), Synset('stand_for.v.04'), Synset('take_a_joke.v.01'), Synset('take_lying_down.v.01')]\n",
      "Synset('encouraging.s.02')\n",
      "[]\n",
      "Synset('load-bearing.s.01')\n",
      "[]\n",
      "Word:  themselves\n"
     ]
    }
   ],
   "source": [
    "#Create array for hyponyms\n",
    "np_hyp = []\n",
    "#Loop through each word\n",
    "for i in fdist.most_common(10):\n",
    "    # Print the word\n",
    "    print('Word: ', i[0])\n",
    "    count=0\n",
    "    \n",
    "    #Print Hyponyms\n",
    "    for j in wn.synsets(i[0]):\n",
    "        print(j)\n",
    "        print(j.hyponyms())\n",
    "        count+=len(j.hyponyms())\n",
    "    np_hyp.append([i[0], count])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create empty array\n",
    "np_1 = []\n",
    "np_2 = []\n",
    "\n",
    "#Get the hyponyms\n",
    "for i in range(10):\n",
    "    np_1.append(np_hyp[i][0])\n",
    "\n",
    "for j in range(10):\n",
    "    np_2.append(np_hyp[j][1])\n",
    "    \n",
    "#Create dataframe\n",
    "df_hyp = pd.DataFrame({'Word':np_1, 'Count':np_2})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Word</th>\n",
       "      <th>Count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>americans</td>\n",
       "      <td>75</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>supporting</td>\n",
       "      <td>42</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>committed</td>\n",
       "      <td>16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>revolution</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>citizens</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>generation</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>president</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>forebears</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>powerful</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>themselves</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         Word  Count\n",
       "2   americans     75\n",
       "8  supporting     42\n",
       "6   committed     16\n",
       "5  revolution      8\n",
       "0    citizens      7\n",
       "3  generation      6\n",
       "1   president      3\n",
       "4   forebears      2\n",
       "7    powerful      0\n",
       "9  themselves      0"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_hyp.sort_values(by=\"Count\", ascending=False) # Sort from highest to lowest hyponyms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Word 'Americans' has the highest number of hyponyms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# G) Reflect on the results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using NLTK, we can extract useful information from a corpus of text. Here, using a few lines of code we were able to analyze 18 different texts in the gutenberg corpus and find the most common words that can tell us a little bit about each text.\n",
    "\n",
    "In the inaugural Kennedy text, we found some of the most common words used in his speech and learned which of those common words has more synonyms and hyponyms. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
